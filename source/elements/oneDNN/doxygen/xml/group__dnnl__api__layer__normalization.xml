<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.17">
  <compounddef id="group__dnnl__api__layer__normalization" kind="group">
    <compoundname>dnnl_api_layer_normalization</compoundname>
    <title>Layer Normalization</title>
    <innerclass refid="structdnnl_1_1layer__normalization__forward" prot="public">dnnl::layer_normalization_forward</innerclass>
    <innerclass refid="structdnnl_1_1layer__normalization__backward" prot="public">dnnl::layer_normalization_backward</innerclass>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para>A primitive to perform layer normalization. Normalization is performed within the last logical dimension of data tensor.</para>
<para>Both forward and backward propagation primitives support in-place operation; that is, src and dst can refer to the same memory for forward propagation, and diff_dst and diff_src can refer to the same memory for backward propagation.</para>
<para>The layer normalization primitives computations can be controlled by specifying different <ref refid="group__dnnl__api__primitives__common_1gad8ef0fcbb7b10cae3d67dd46892002be" kindref="member">dnnl::normalization_flags</ref> values. For example, layer normalization forward propagation can be configured to either compute the mean and variance or take them as arguments. It can either perform scaling and shifting using gamma and beta parameters or not. Optionally, it can also perform a fused ReLU, which in case of training would also require a workspace. </para>
    </detaileddescription>
  </compounddef>
</doxygen>
